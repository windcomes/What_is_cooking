{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "feature1_clfs",
      "version": "0.3.2",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "metadata": {
        "id": "OQBH7wHmezPP",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import json\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.decomposition import TruncatedSVD\n",
        "\n",
        "from sklearn import svm\n",
        "import time\n",
        "\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "from sklearn.linear_model import PassiveAggressiveClassifier\n",
        "from sklearn.datasets import make_classification"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "_D9xeLPse2Pe",
        "colab_type": "code",
        "outputId": "f6f3eaa4-a9ba-4f4c-ae2c-51b954317a27",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 701
        }
      },
      "cell_type": "code",
      "source": [
        "with open('train.json', encoding='utf-8') as f:\n",
        "    d = json.load(f)\n",
        "    f.close()\n",
        "\n",
        "data_all = pd.DataFrame(d)\n",
        "\n",
        "def num_ingre_each_recipe(list_ingrs_each_recipe):\n",
        "    '''\n",
        "    This method is to count the number of ingredients of each recipe\n",
        "    '''\n",
        "    return len(list_ingrs_each_recipe)\n",
        "\n",
        "\n",
        "data_all['num_ingre_contained'] = data_all['ingredients'].apply(num_ingre_each_recipe)\n",
        "\n",
        "# Only choose the recipes containing more than 3 ingres.\n",
        "data_all = data_all[data_all.num_ingre_contained >= 3]\n",
        "\n",
        "print(data_all.shape)  # the total amount of data is 39774\n",
        "\n",
        "def datasets_cleaning(list_input):\n",
        "    seven_up = re.compile(r\"^7\\sUp\")  # the Regular Expression of '7 Up'\n",
        "    hype = re.compile(r\"-\")\n",
        "\n",
        "    deleted_str = []\n",
        "    deleted_str.append(re.compile(r\"\\(.*\\)\"))\n",
        "    deleted_str.append(re.compile(r\"%\"))\n",
        "    deleted_str.append(re.compile(r\"/\"))\n",
        "    deleted_str.append(re.compile(r\"!\"))\n",
        "    deleted_str.append(re.compile(r\"’\"))\n",
        "    deleted_str.append(re.compile(r\"\\.\"))\n",
        "    deleted_str.append(re.compile(r\"\\d+\\s\"))\n",
        "    deleted_str.append(re.compile(r\"\\b.*®\"))\n",
        "    deleted_str.append(re.compile(r\",\"))\n",
        "    deleted_str.append(re.compile(r\"&\"))\n",
        "    deleted_str.append(re.compile(r\"\\b.*™\"))\n",
        "    deleted_str.append(re.compile(r\"'\"))\n",
        "\n",
        "    useless_words = [\"fat\", \"free\", \"ounc\", \"oz\", \"fine\", \"finely\", \"superfine\", \"crushed\", \"crush\", \"cut\", \n",
        "                     \"up\", \"age\", \"fashioned\", \"press\", \"refined\", \"squeeze\", \"refrigerated\", \"diced\", \n",
        "                     \"processed\",\"nonfat\", \"packed\", \"firmly\", \"loosely\", \"gluten\", \"low\", \"high\", \"less\", \n",
        "                     \"sodium\",\"reduced\",\"organic\", \"store bought\", \"of\", \"the\", \"semi\", \"whole\", \"reduced\",\n",
        "                     \"light\", \"softened\",\"ground\", \"fresh\", \"natural\", \"flavored\", \"plain\", \"unsweetened\",\n",
        "                     \"vegan\",\"drained\",\"bags\", \"squirt\", \"originals\", \"flavoured\", \"cook\"]\n",
        "\n",
        "    Brand_names = [\"Bertolli\", \"Crocker\", \"Conimex\", \"Colman\", \"Crystal Farms\", \"DeLallo\", \"Domino\",\n",
        "                   \"Doritos\", \"Earth Balance\", \"Elmlea\", \"Estancia\", \"Fisher\", \"Flora\", \"Foster Farms\",\n",
        "                   \"Gourmet Garden\", \"Goya\", \"Green Giant\", \"Heinz\", \"Hellmann\", \"Hidden Valley\",\n",
        "                   \"Honeysuckle White\", \"Imperial\", \"JOHNSONVILLE\", \"Jack Daniels\", \"Johnsonville\",\n",
        "                   \"Jimmy Dean\", \"KRAFT\", \"Knorr\", \"Lipton\", \"Manischewitz\", \"McCormick\", \"Mazola\",\n",
        "                   \"Old El Paso\", \"Pillsbury\", \"Progresso\", \"Pure Wesson\", \"Ragu\", \"San Marzano\",\n",
        "                   \"Sargento\", \"Soy Vay\", \"Spice Islands\", \"Taco BELL\", \"Truvía\", \"Uncle Ben\",\n",
        "                   \"Uncle Bens\", \"Velveeta\", \"Wish Bone\", \"Yoplait\", \"Zatarain\", \"Best Food\", \"Breyers\",\n",
        "                   \"Campbell\", \"Hidden Valley\", \"Knorr\", \"McCormick\", \"Mizkan\", \"Progresso\",\n",
        "                   \"Frank\", \"Red Gold\"]\n",
        "\n",
        "    useless_words = [r\"%s\\b\" % useless_words[j]\n",
        "                     for j in range(len(useless_words))]\n",
        "    # print(useless_words)\n",
        "    Brand_names = [r\"%s\\b\" % Brand_names[i].lower()\n",
        "                   for i in range(len(Brand_names))]\n",
        "\n",
        "    deleted_str = deleted_str + useless_words + Brand_names\n",
        "\n",
        "    for string in range(len(list_input)):\n",
        "        list_input[string] = re.sub(seven_up, \"7up\", list_input[string])\n",
        "        list_input[string] = re.sub(hype, \" \", list_input[string])\n",
        "        list_input[string] = re.sub(r\"_\", \" \", list_input[string])\n",
        "        list_input[string] = list_input[string].lower()\n",
        "        for del_str in deleted_str:\n",
        "            list_input[string] = re.sub(del_str, \" \", list_input[string])\n",
        "        list_input[string] = re.sub(r\"^\\s+\", \"\", list_input[string])\n",
        "        list_input[string] = re.sub(r\"\\s+$\", \"\", list_input[string])\n",
        "        list_input[string] = re.sub(r\"\\s+\", \"_\", list_input[string])\n",
        "\n",
        "    return list_input\n",
        "\n",
        "\n",
        "data_all['ingredients'] = data_all['ingredients'].apply(datasets_cleaning)\n",
        "\n",
        "\n",
        "# now we seperate the dataset into train, valid, test\n",
        "y_all = data_all['cuisine'].tolist()\n",
        "X_all = data_all['ingredients'].tolist()\n",
        "\n",
        "Xtrain, Xtestval, ytrain, ytestval = train_test_split(X_all,y_all, test_size = 0.2, random_state = 42)\n",
        "Xtest, Xval, ytest, yval = train_test_split(Xtestval, ytestval, test_size = 0.5, random_state = 42)\n",
        "\n",
        "data_train = pd.DataFrame(columns=['cuisine','ingredients'])\n",
        "data_train['cuisine'] = ytrain\n",
        "data_train['ingredients'] = Xtrain # Creat a DataFrame based on train data(size:31647 *2)\n",
        "\n",
        "data_train['num_ingre_contained'] = data_train['ingredients'].apply(num_ingre_each_recipe)\n",
        "data_train['ingre_string'] = data_train['ingredients'].str.join(' ')\n",
        "\n",
        "list_corpus = data_train['ingre_string'].tolist()\n",
        "list_corpus[0:3]\n",
        "\n",
        "vectorizer = CountVectorizer()\n",
        "vectorizer.fit(list_corpus)\n",
        "Xtrain = vectorizer.transform(data_train['ingre_string']).toarray()\n",
        "feature_names = np.array(vectorizer.get_feature_names())\n",
        "print(feature_names)\n",
        "print(len(feature_names)) \n",
        "\n",
        "\n",
        "print(Xtrain)\n",
        "print(np.shape(Xtrain))\n",
        "print(np.array(ytrain))\n",
        "print(np.shape(ytrain))\n",
        "\n",
        "\n",
        "# we use this method to update our splited words and then combine them into a sentence once again.\n",
        "data_val = pd.DataFrame(columns=['ingredients'])\n",
        "data_val['ingredients'] = Xval \n",
        "data_val['num_ingre_contained'] = data_val['ingredients'].apply(num_ingre_each_recipe)\n",
        "data_val['ingre_string'] = data_val['ingredients'].str.join(' ')\n",
        "Xval = vectorizer.transform(data_val['ingre_string']).toarray()\n",
        "\n",
        "\n",
        "data_test = pd.DataFrame(columns=['ingredients'])\n",
        "data_test['ingredients'] = Xtest \n",
        "data_val['num_ingre_contained'] = data_val['ingredients'].apply(num_ingre_each_recipe)\n",
        "data_test['ingre_string'] = data_test['ingredients'].str.join(' ')\n",
        "Xtest = vectorizer.transform(data_test['ingre_string']).toarray()\n",
        "\n",
        "print(Xval)\n",
        "print(np.shape(Xval))\n",
        "print(np.array(yval))\n",
        "print(np.shape(yval))\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.decomposition import TruncatedSVD\n",
        "tsvd = TruncatedSVD(n_components=200, random_state = 42)\n",
        "\n",
        "Xtrain_svd = tsvd.fit(Xtrain).transform(Xtrain)\n",
        "Xtest_svd = tsvd.transform(Xtest)\n",
        "Xval_svd = tsvd.transform(Xval)\n",
        "\n",
        "print(Xtrain_svd)\n",
        "print(np.shape(Xtrain_svd))\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(39559, 4)\n",
            "['a_taste_thai_rice_noodles' 'abalone' 'abbamele' ...\n",
            " 'ziti_pasta_and_drain' 'zucchini' 'zucchini_blossoms']\n",
            "5736\n",
            "[[0 0 0 ... 0 0 0]\n",
            " [0 0 0 ... 0 0 0]\n",
            " [0 0 0 ... 0 0 0]\n",
            " ...\n",
            " [0 0 0 ... 0 0 0]\n",
            " [0 0 0 ... 0 0 0]\n",
            " [0 0 0 ... 0 0 0]]\n",
            "(31647, 5736)\n",
            "['southern_us' 'mexican' 'british' ... 'italian' 'japanese' 'southern_us']\n",
            "(31647,)\n",
            "[[0 0 0 ... 0 0 0]\n",
            " [0 0 0 ... 0 0 0]\n",
            " [0 0 0 ... 0 0 0]\n",
            " ...\n",
            " [0 0 0 ... 0 0 0]\n",
            " [0 0 0 ... 0 0 0]\n",
            " [0 0 0 ... 0 0 0]]\n",
            "(3956, 5736)\n",
            "['chinese' 'italian' 'russian' ... 'italian' 'italian' 'french']\n",
            "(3956,)\n",
            "[[ 0.53527582 -0.44411132 -0.21906304 ... -0.04335137  0.33862042\n",
            "   0.12597848]\n",
            " [ 1.60547664 -0.35081945 -0.01419527 ... -0.08386646 -0.26094328\n",
            "   0.29388734]\n",
            " [ 1.76422587 -0.34706205 -0.19966902 ... -0.00374093 -0.0494686\n",
            "  -0.05759293]\n",
            " ...\n",
            " [ 0.00870534 -0.00372915  0.00192617 ... -0.03469206 -0.00734346\n",
            "  -0.0079372 ]\n",
            " [ 0.677149   -0.42375212  0.90784308 ...  0.00789553  0.0081455\n",
            "  -0.05333466]\n",
            " [ 0.22518311  0.20697599  0.14519699 ... -0.11855092  0.01217291\n",
            "  -0.07900258]]\n",
            "(31647, 200)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "5e2r3uuZec1F",
        "colab_type": "code",
        "outputId": "0d790dbc-88f6-4868-b948-f3d6bede4816",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "cell_type": "code",
      "source": [
        "start = time.clock()\n",
        "clf = svm.SVC( gamma=0.2,C=0.7, decision_function_shape='ovo')\n",
        "clf.fit(Xtrain_svd, ytrain)\n",
        "elapsed = (time.clock() - start)\n",
        "print(\"Time used:\",elapsed)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Time used: 390.762283\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "V-UdTvNkBfno",
        "colab_type": "code",
        "outputId": "1152196b-7446-4afd-f593-2638674f23cf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "cell_type": "code",
      "source": [
        "start = time.clock()\n",
        "clf = LogisticRegression(random_state=0,solver='lbfgs',multi_class='multinomial',max_iter=2000).fit(Xtrain_svd,ytrain)\n",
        "elapsed = (time.clock() - start)\n",
        "print(\"Time used:\",elapsed)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Time used: 44.28662600000007\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "Vk8l1RP-D9b_",
        "colab_type": "code",
        "outputId": "6be3534f-8db7-4e90-b475-9fcd303193de",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        }
      },
      "cell_type": "code",
      "source": [
        "start = time.clock()\n",
        "clf = PassiveAggressiveClassifier(C=0.1,max_iter=2000, tol=1e-3,shuffle=False,)\n",
        "# clf.fit(Xtrain_svd, ytrain)\n",
        "print(Xtrain.shape)\n",
        "clf.fit(Xtrain, ytrain)\n",
        "\n",
        "elapsed = (time.clock() - start)\n",
        "print(\"Time used:\",elapsed)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(31647, 5736)\n",
            "Time used: 197.3306990000001\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "NSjpDo_wjx62",
        "colab_type": "code",
        "outputId": "b33a2b84-28d1-41a2-db18-f5d1b07beccc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        }
      },
      "cell_type": "code",
      "source": [
        "# X_val_svd = svd.fit(Xtrain).transform(Xval)\n",
        "p_result=clf.predict(Xtest_svd)\n",
        "\n",
        "\n",
        "print(np.sum(ytest==p_result)/len(p_result))\n",
        "# print(np.bincount(yval))\n",
        "print(clf.score(Xtest_svd,ytest))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.6870576339737108\n",
            "0.6870576339737108\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}