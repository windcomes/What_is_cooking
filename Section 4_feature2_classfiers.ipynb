{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "feature2_classfiers",
      "version": "0.3.2",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "metadata": {
        "id": "J2OP9wUPB9UA",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import json\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "from sklearn.decomposition import TruncatedSVD\n",
        "\n",
        "from sklearn import svm\n",
        "import time\n",
        "\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "from sklearn.linear_model import PassiveAggressiveClassifier\n",
        "from sklearn.datasets import make_classification"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "mLHJ8GeLCFh9",
        "colab_type": "code",
        "outputId": "2f2dba2f-e980-430f-e5cf-81d12b7509c9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 881
        }
      },
      "cell_type": "code",
      "source": [
        "with open('train.json', encoding='utf-8') as f:\n",
        "    d = json.load(f)\n",
        "    f.close()\n",
        "\n",
        "data_all = pd.DataFrame(d)\n",
        "\n",
        "def num_ingre_each_recipe(list_ingrs_each_recipe):\n",
        "    '''\n",
        "    This method is to count the number of ingredients of each recipe\n",
        "    '''\n",
        "    return len(list_ingrs_each_recipe)\n",
        "\n",
        "\n",
        "data_all['num_ingre_contained'] = data_all['ingredients'].apply(num_ingre_each_recipe)\n",
        "\n",
        "# Only choose the recipes containing more than 3 ingredients.\n",
        "data_all = data_all[data_all.num_ingre_contained >= 3]\n",
        "\n",
        "print(data_all.shape)  # the total amount of data is 39774\n",
        "\n",
        "def datasets_cleaning(list_input):\n",
        "    seven_up = re.compile(r\"^7\\sUp\")  # the Regular Expression of '7 Up'\n",
        "    hype = re.compile(r\"-\")\n",
        "\n",
        "    deleted_str = []\n",
        "    deleted_str.append(re.compile(r\"\\(.*\\)\"))\n",
        "    deleted_str.append(re.compile(r\"%\"))\n",
        "    deleted_str.append(re.compile(r\"/\"))\n",
        "    deleted_str.append(re.compile(r\"!\"))\n",
        "    deleted_str.append(re.compile(r\"’\"))\n",
        "    deleted_str.append(re.compile(r\"\\.\"))\n",
        "    deleted_str.append(re.compile(r\"\\d+\\s\"))\n",
        "    deleted_str.append(re.compile(r\"\\b.*®\"))\n",
        "    deleted_str.append(re.compile(r\",\"))\n",
        "    deleted_str.append(re.compile(r\"&\"))\n",
        "    deleted_str.append(re.compile(r\"\\b.*™\"))\n",
        "    deleted_str.append(re.compile(r\"'\"))\n",
        "\n",
        "    useless_words = [\"fat\", \"free\", \"ounc\", \"oz\", \"fine\", \"finely\", \"superfine\", \"crushed\", \"crush\", \"cut\", \n",
        "                     \"up\", \"age\", \"fashioned\", \"press\", \"refined\", \"squeeze\", \"refrigerated\", \"diced\", \n",
        "                     \"processed\",\"nonfat\", \"packed\", \"firmly\", \"loosely\", \"gluten\", \"low\", \"high\", \"less\", \n",
        "                     \"sodium\",\"reduced\",\"organic\", \"store bought\", \"of\", \"the\", \"semi\", \"whole\", \"reduced\",\n",
        "                     \"light\", \"softened\",\"ground\", \"fresh\", \"natural\", \"flavored\", \"plain\", \"unsweetened\",\n",
        "                     \"vegan\",\"drained\",\"bags\", \"squirt\", \"originals\", \"flavoured\", \"cook\"]\n",
        "\n",
        "    Brand_names = [\"Bertolli\", \"Crocker\", \"Conimex\", \"Colman\", \"Crystal Farms\", \"DeLallo\", \"Domino\",\n",
        "                   \"Doritos\", \"Earth Balance\", \"Elmlea\", \"Estancia\", \"Fisher\", \"Flora\", \"Foster Farms\",\n",
        "                   \"Gourmet Garden\", \"Goya\", \"Green Giant\", \"Heinz\", \"Hellmann\", \"Hidden Valley\",\n",
        "                   \"Honeysuckle White\", \"Imperial\", \"JOHNSONVILLE\", \"Jack Daniels\", \"Johnsonville\",\n",
        "                   \"Jimmy Dean\", \"KRAFT\", \"Knorr\", \"Lipton\", \"Manischewitz\", \"McCormick\", \"Mazola\",\n",
        "                   \"Old El Paso\", \"Pillsbury\", \"Progresso\", \"Pure Wesson\", \"Ragu\", \"San Marzano\",\n",
        "                   \"Sargento\", \"Soy Vay\", \"Spice Islands\", \"Taco BELL\", \"Truvía\", \"Uncle Ben\",\n",
        "                   \"Uncle Bens\", \"Velveeta\", \"Wish Bone\", \"Yoplait\", \"Zatarain\", \"Best Food\", \"Breyers\",\n",
        "                   \"Campbell\", \"Hidden Valley\", \"Knorr\", \"McCormick\", \"Mizkan\", \"Progresso\",\n",
        "                   \"Frank\", \"Red Gold\"]\n",
        "\n",
        "    useless_words = [r\"%s\\b\" % useless_words[j]\n",
        "                     for j in range(len(useless_words))]\n",
        "    # print(useless_words)\n",
        "    Brand_names = [r\"%s\\b\" % Brand_names[i].lower()\n",
        "                   for i in range(len(Brand_names))]\n",
        "\n",
        "    deleted_str = deleted_str + useless_words + Brand_names\n",
        "\n",
        "    for string in range(len(list_input)):\n",
        "        list_input[string] = re.sub(seven_up, \"7up\", list_input[string])\n",
        "        list_input[string] = re.sub(hype, \" \", list_input[string])\n",
        "        list_input[string] = re.sub(r\"_\", \" \", list_input[string])\n",
        "        list_input[string] = list_input[string].lower()\n",
        "        for del_str in deleted_str:\n",
        "            list_input[string] = re.sub(del_str, \" \", list_input[string])\n",
        "        list_input[string] = re.sub(r\"^\\s+\", \"\", list_input[string])\n",
        "        list_input[string] = re.sub(r\"\\s+$\", \"\", list_input[string])\n",
        "        list_input[string] = re.sub(r\"\\s+\", \"_\", list_input[string])\n",
        "\n",
        "    return list_input\n",
        "\n",
        "\n",
        "data_all['ingredients'] = data_all['ingredients'].apply(datasets_cleaning)\n",
        "\n",
        "# now we seperate the dataset into train, valid, test\n",
        "y_all = data_all['cuisine'].tolist()\n",
        "X_all = data_all['ingredients'].tolist()\n",
        "\n",
        "Xtrain, Xtestval, ytrain, ytestval = train_test_split(X_all,y_all, test_size = 0.2, random_state = 42)\n",
        "Xtest, Xval, ytest, yval = train_test_split(Xtestval, ytestval, test_size = 0.5, random_state = 42)\n",
        "\n",
        "data_train = pd.DataFrame(columns=['cuisine','ingredients'])\n",
        "data_train['cuisine'] = ytrain\n",
        "data_train['ingredients'] = Xtrain # Creat a DataFrame based on train data(size:31647 *2)\n",
        "\n",
        "data_train['num_ingre_contained'] = data_train['ingredients'].apply(num_ingre_each_recipe)\n",
        "data_train['ingre_string'] = data_train['ingredients'].str.join(' ')\n",
        "\n",
        "\n",
        "list_corpus = data_train['ingre_string'].tolist()\n",
        "\n",
        "vectorizer = TfidfVectorizer()\n",
        "vectorizer.fit(list_corpus)\n",
        "Xtrain = vectorizer.transform(data_train['ingre_string']).toarray()\n",
        "feature_names = np.array(vectorizer.get_feature_names())\n",
        "print(feature_names)\n",
        "print(len(feature_names)) \n",
        "\n",
        "\n",
        "print(Xtrain)\n",
        "print(np.shape(Xtrain))\n",
        "print(np.array(ytrain))\n",
        "print(np.shape(ytrain))\n",
        "# the size of array is 31647*2387 ,31647 is the size of train data, 2385 is the number of features.\n",
        "\n",
        "\n",
        "# we use this method to update our splited words and then combine them into a sentence once again.\n",
        "data_val = pd.DataFrame(columns=['ingredients'])\n",
        "data_val['ingredients'] = Xval \n",
        "data_val['num_ingre_contained'] = data_val['ingredients'].apply(num_ingre_each_recipe)\n",
        "data_val['ingre_string'] = data_val['ingredients'].str.join(' ')\n",
        "Xval = vectorizer.transform(data_val['ingre_string']).toarray()\n",
        "\n",
        "\n",
        "data_test = pd.DataFrame(columns=['ingredients'])\n",
        "data_test['ingredients'] = Xtest \n",
        "data_val['num_ingre_contained'] = data_val['ingredients'].apply(num_ingre_each_recipe)\n",
        "data_test['ingre_string'] = data_test['ingredients'].str.join(' ')\n",
        "Xtest = vectorizer.transform(data_test['ingre_string']).toarray()\n",
        "\n",
        "\n",
        "print(Xval)\n",
        "print(np.shape(Xval))\n",
        "print(np.array(yval))\n",
        "print(np.shape(yval))\n",
        "\n",
        "print(Xtest)\n",
        "print(np.shape(Xtest))\n",
        "print(np.array(ytest))\n",
        "print(np.shape(ytest))\n",
        "\n",
        "\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.decomposition import TruncatedSVD\n",
        "tsvd = TruncatedSVD(n_components=200, random_state = 42)\n",
        "\n",
        "Xtrain_svd = tsvd.fit(Xtrain).transform(Xtrain)\n",
        "Xtest_svd = tsvd.transform(Xtest)\n",
        "Xval_svd = tsvd.transform(Xval)\n",
        "\n",
        "print(Xtrain_svd)\n",
        "print(np.shape(Xtrain_svd))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(39559, 4)\n",
            "['a_taste_thai_rice_noodles' 'abalone' 'abbamele' ...\n",
            " 'ziti_pasta_and_drain' 'zucchini' 'zucchini_blossoms']\n",
            "5736\n",
            "[[0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " ...\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]]\n",
            "(31647, 5736)\n",
            "['southern_us' 'mexican' 'british' ... 'italian' 'japanese' 'southern_us']\n",
            "(31647,)\n",
            "[[0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " ...\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]]\n",
            "(3956, 5736)\n",
            "['chinese' 'italian' 'russian' ... 'italian' 'italian' 'french']\n",
            "(3956,)\n",
            "[[0.         0.         0.         ... 0.         0.         0.        ]\n",
            " [0.         0.         0.         ... 0.         0.         0.        ]\n",
            " [0.         0.         0.         ... 0.         0.25572739 0.        ]\n",
            " ...\n",
            " [0.         0.         0.         ... 0.         0.20120954 0.        ]\n",
            " [0.         0.         0.         ... 0.         0.         0.        ]\n",
            " [0.         0.         0.         ... 0.         0.         0.        ]]\n",
            "(3956, 5736)\n",
            "['chinese' 'indian' 'italian' ... 'mexican' 'british' 'thai']\n",
            "(3956,)\n",
            "[[ 0.10103338 -0.04490946 -0.05959988 ... -0.00043458  0.0123596\n",
            "  -0.05386894]\n",
            " [ 0.17619503 -0.06287786  0.01186777 ...  0.01396078  0.01359158\n",
            "  -0.04309299]\n",
            " [ 0.23057816 -0.05792898 -0.03112097 ... -0.00681536  0.03253199\n",
            "   0.03310499]\n",
            " ...\n",
            " [ 0.00673122 -0.00188402 -0.00200171 ...  0.01089356  0.00094447\n",
            "   0.0092215 ]\n",
            " [ 0.13000443 -0.06023984  0.12658635 ...  0.02560276 -0.02979102\n",
            "   0.00902171]\n",
            " [ 0.08163789  0.06902614  0.00426302 ...  0.0098669   0.01218502\n",
            "   0.02959644]]\n",
            "(31647, 200)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "fxWzJISZCaXB",
        "colab_type": "code",
        "outputId": "1d90f7e9-1d3e-481e-9154-fd403bcf11c4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "cell_type": "code",
      "source": [
        "start = time.clock()\n",
        "clf = LogisticRegression(random_state=0,solver='lbfgs',multi_class='multinomial',max_iter=2000).fit(Xtrain_svd,ytrain)\n",
        "elapsed = (time.clock() - start)\n",
        "print(\"Time used:\",elapsed)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Time used: 37.408456\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "O0cZE865DS3-",
        "colab_type": "code",
        "outputId": "4abd3619-9de5-449c-cfb3-30e6694cc925",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        }
      },
      "cell_type": "code",
      "source": [
        "start = time.clock()\n",
        "clf = PassiveAggressiveClassifier(C=0.1,max_iter=2000, tol=1e-3,shuffle=False,)\n",
        "# clf.fit(Xtrain_svd, ytrain)\n",
        "\n",
        "print(Xtrain.shape)\n",
        "clf.fit(Xtrain, ytrain)\n",
        "\n",
        "elapsed = (time.clock() - start)\n",
        "print(\"Time used:\",elapsed)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(31647, 5736)\n",
            "Time used: 180.15101099999998\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "ZRWVAKqBCfni",
        "colab_type": "code",
        "outputId": "bf61ae8b-6bd2-4e44-826d-44a880ec0db5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        }
      },
      "cell_type": "code",
      "source": [
        "p_result=clf.predict(Xtest)\n",
        "\n",
        "\n",
        "print(np.sum(ytest==p_result)/len(p_result))\n",
        "# print(np.bincount(yval))\n",
        "print(clf.score(Xtest,ytest))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.7762891809908999\n",
            "0.7762891809908999\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}