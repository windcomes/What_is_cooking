{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "feature3_clfs",
      "version": "0.3.2",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "metadata": {
        "id": "kU38KLHL5xKW",
        "colab_type": "code",
        "outputId": "75fe77ce-74ba-4201-d825-4dc08eb586dd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 125
        }
      },
      "cell_type": "code",
      "source": [
        "import json\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "import nltk\n",
        "nltk.download(\"wordnet\")\n",
        "nltk.download(\"averaged_perceptron_tagger\")\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.decomposition import TruncatedSVD\n",
        "\n",
        "from sklearn import svm\n",
        "import time\n",
        "\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "from sklearn.linear_model import PassiveAggressiveClassifier\n",
        "from sklearn.datasets import make_classification"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "HFOskcH357x_",
        "colab_type": "code",
        "outputId": "c2dccdcb-f298-45cd-c78b-9888ae675c3e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 649
        }
      },
      "cell_type": "code",
      "source": [
        "\n",
        "with open('train.json', encoding='utf-8') as f:\n",
        "    d = json.load(f)\n",
        "    f.close()\n",
        "\n",
        "data_all = pd.DataFrame(d)\n",
        "\n",
        "\n",
        "def num_ingre_each_recipe(list_ingrs_each_recipe):\n",
        "    '''\n",
        "    This method is to count the number of ingredients of each recipe\n",
        "    '''\n",
        "    return len(list_ingrs_each_recipe)\n",
        "\n",
        "\n",
        "data_all['num_ingredients_contained'] = data_all['ingredients'].apply(\n",
        "    num_ingre_each_recipe)\n",
        "print(data_all.shape)  # the total amount of data is 39774\n",
        "\n",
        "# Only choose the recipes containing more than 3 ingres.\n",
        "data_all = data_all[data_all.num_ingredients_contained >= 3]\n",
        "\n",
        "# there are some special words like numbers are meaningless.\n",
        "# Hence, the first thing is to remove these from the datasets.\n",
        "# other consideration can all be solved by sklearn package\n",
        "# (Make sure the word is in lower case; Delete symbols; Delete letters standing alone; Delete double spaces)\n",
        "\n",
        "numbers = re.compile(r\"\\d\")   # the Regular Expression of numbers\n",
        "seven_up = re.compile(r\"^7\\sUp\")  # the Regular Expression of '7 Up'\n",
        "\n",
        "\n",
        "def datasets_cleaning(list_input):\n",
        "    for string in range(len(list_input)):\n",
        "        list_input[string] = re.sub(numbers, \"\", list_input[string])\n",
        "        list_input[string] = re.sub(seven_up, \"7up\", list_input[string])\n",
        "    return list_input\n",
        "\n",
        "\n",
        "#data_all['ingredients'] = data_all['ingredients'].apply(datasets_cleaning)\n",
        "    \n",
        "# now we seperate the dataset into train, valid, test\n",
        "y_all = data_all['cuisine'].tolist()\n",
        "X_all = data_all['ingredients'].tolist()\n",
        "\n",
        "Xtrain, Xtestval, ytrain, ytestval = train_test_split(X_all,y_all, test_size = 0.2, random_state = 42)\n",
        "Xtest, Xval, ytest, yval = train_test_split(Xtestval, ytestval, test_size = 0.5, random_state = 42)\n",
        "\n",
        "data_train = pd.DataFrame(columns=['cuisine','ingredients'])\n",
        "data_train['cuisine'] = ytrain\n",
        "data_train['ingredients'] = Xtrain # Creat a DataFrame based on train data(size:31647 *2)\n",
        "\n",
        "dict_cuisine = {}\n",
        "for key in ytrain:\n",
        "    dict_cuisine[key] = dict_cuisine.get(key, 0) + 1\n",
        "print(dict_cuisine)  # the distribution of cuisine in ytrain\n",
        "\n",
        "\n",
        "# then we want to split the name of each ingredients into several single words.\n",
        "def tokenize_list(list_input):\n",
        "    new_list = [val.split(\" \") for val in list_input]\n",
        "    list_get = [item for sublist in new_list for item in sublist]\n",
        "    return list_get\n",
        "\n",
        "\n",
        "data_train['ingre_splited'] = data_train['ingredients'].apply(tokenize_list)\n",
        "\n",
        "\n",
        "\n",
        "# there are lot of similar words after spliting the name of ingredients like \"apple\" and \"apples\"\n",
        "# we use stemming algorithm to get the same stem\n",
        "\n",
        "from nltk.stem import PorterStemmer\n",
        "porter = PorterStemmer()\n",
        "\n",
        "\n",
        "def token_stemming(list_input):\n",
        "    list_get = [porter.stem(word) for word in list_input]\n",
        "    return list_get\n",
        "\n",
        "\n",
        "# we use this method to update our splited words and then combine them into a sentence.\n",
        "data_train['ingre_splited'] = data_train['ingre_splited'].apply(token_stemming)\n",
        "data_train['ingre_string'] = data_train['ingre_splited'].str.join(' ')\n",
        "\n",
        "\n",
        "# we generate the corpus based on the train data.\n",
        "list_corpus = data_train['ingre_string'].tolist()\n",
        "\n",
        "\n",
        "# Some words are stop words like 'a' 'is' 'or' 'not', we can then delete them from features\n",
        "# Some words are meaningless adj., we can also delete them\n",
        "# Some words are the brand names of ingredient, we can delete them as well\n",
        "\n",
        "# add these meaningless words into the set of stop words, deleting all of them in the following TD-IDF model\n",
        "\n",
        "\n",
        "def add_deleted_words():\n",
        "    '''\n",
        "    Some words are stop words like 'a' 'is' 'or' 'not', we can then delete them from features\n",
        "    Some words are meaningless adj., we can also delete them\n",
        "    Some words are the brand names of ingredient, we can delete them as well\n",
        "    add these meaningless words into the set of stop words, deleting all of them in the following TD-IDF model\n",
        "    '''\n",
        "\n",
        "    # Here are all the words I(Quentin) could find to try and simplify the ingredient names\n",
        "    list_delete_words = [\"fat\", \"free\", \"oz\", \"fine\", \"finely\", \"superfine\", \"crushed\", \"crush\", \"cut\", \"up\", \"age\",\n",
        "                   \"fashioned\", \"press\", \"refined\", \"squeeze\", \"refrigerated\", \"smoked\", \"sweet\", \"diced\", \"processed\",\n",
        "                   \"nonfat\", \"packed\", \"firmly\", \"loosely\", \"gluten\", \"low\", \"high\", \"less\", \"sodium\", \"reduced\",\n",
        "                   \"organic\", \"store bought\", \"of\", \"the\", \"semi\", \"condensed\", \"whole\", \"reduced\", \"light\", \"softened\"\n",
        "                   \"ground\", \"fresh\", \"black\", \"natural\", \"flavored\", \"plain\", \"unsweetened\", \"vegan\", \"nonfat\"]\n",
        "    list_delete_words = tokenize_list(list_delete_words) # Split the Brand names.\n",
        "    \n",
        "    # All the brand names I(Quentin) found, we should delete the partial names including the name of ingredient.\n",
        "    # for example, we only delete \"Vay\" rather than\"Soy Vay\",  delete \"bell\" rather than\"Taco bell\"\n",
        "    Brand_names = [\"Bertolli\", \"Crocker\", \"Conimex\", \"Colman\", \"Crystal Farms\", \"DeLallo\", \"Domino\",\n",
        "                   \"Doritos\", \"Earth Balance\", \"Elmlea\", \"Estancia\", \"Fisher\", \"Flora\", \"Foster Farms\",\n",
        "                   \"Gourmet Garden\", \"Goya\", \"Green Giant\", \"Heinz\", \"Hellmann\", \"Hidden Valley\",\n",
        "                   \"Honeysuckle White\", \"Imperial\", \"JOHNSONVILLE\", \"Jack Daniels\", \"Johnsonville\",\n",
        "                   \"Jimmy Dean\", \"KRAFT\", \"Knorr\", \"Lipton\", \"Manischewitz\", \"McCormick\", \"Mazola\",\n",
        "                   \"Old El Paso\", \"Pillsbury\", \"Progresso\", \"Pure Wesson\", \"Ragu\", \"San Marzano\",\n",
        "                   \"Sargento\", \"Vay\", \"Spice Islands\", \"BELL\", \"Truv√≠a\", \"Uncle Ben\",\n",
        "                   \"Velveeta\", \"Wish Bone\", \"Yoplait\", \"Zatarain\", \"Best Food\", \"Breyers\",\n",
        "                   \"Campbell\", \"Hidden Valley\", \"Knorr\", \"McCormick\", \"Mizkan\", \"Progresso\",\n",
        "                   \"Frank\", \"Red Gold\"]\n",
        "    \n",
        "    Brand_names = tokenize_list(Brand_names) # Split the Brand names.\n",
        "    Brand_names = [Brand_names[i].lower() for i in range(len(Brand_names))]  # lower case brand names\n",
        "    \n",
        "    # The following is the original codes, the problem is that the list_of_words[i] is actually a sing word\n",
        "    #------------------------------------------------------------------------------------------------------\n",
        "    # which can not match full words of brand, so we need to also split the brand names into single words.\n",
        "    #def RemoveWords(list_of_words):\n",
        "        #for i in range(len(list_of_words)):\n",
        "            #for word_remove in dict_remove:          \n",
        "                #list_of_words[i]  = re.sub(\"\\\\b\" + word_remove + \"\\\\b\", \"\", list_of_words[i]) #Only match full words\n",
        "        #return(list_of_words)\n",
        "    #unique_words = RemoveWords(unique_words)\n",
        "    #------------------------------------------------------------------------------------------------------\n",
        "\n",
        "    from sklearn.feature_extraction.text import ENGLISH_STOP_WORDS\n",
        "    #  use package including all stop words as part of our words waited for deleting\n",
        "\n",
        "    list_delete_words = list_delete_words + Brand_names + [string for string in ENGLISH_STOP_WORDS]\n",
        "    \n",
        "    list_delete_words_stem = [porter.stem(word) for word in list_delete_words]\n",
        "    \n",
        "    list_delete_words = list_delete_words + list_delete_words_stem\n",
        "    \n",
        "    return sorted(list(set(list_delete_words)))\n",
        "\n",
        "delete_words = add_deleted_words()\n",
        "\n",
        "\n",
        "vectorizer = TfidfVectorizer(stop_words=delete_words) # it comes from the built-in method in skilearn. TD-IDF\n",
        "\n",
        "vectorizer.fit(list_corpus)\n",
        "# from the output of this code, we can see the some defult setting\n",
        "# lowercase==True means it will automatically transform all sing words into lower case\n",
        "# token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b' means it will only contain the words rather than a sing letter or some special symbol.\n",
        "\n",
        "vector = vectorizer.transform(data_train['ingre_string'])\n",
        "feature_names = np.array(vectorizer.get_feature_names())\n",
        "print(feature_names)\n",
        "print(len(feature_names)) # count how many feature we constructed by the bag of words model(TD-IDF).\n",
        "\n",
        "\n",
        "sorted_by_idf = np.argsort(vectorizer.idf_) \n",
        "# get the index of features which are ordered by their idf values\n",
        "\n",
        "print(feature_names[sorted_by_idf[:30]]) \n",
        "# idf values means how frequent it exists in recipes. now we present the most frequent 30 features.\n",
        "\n",
        "# As these features are so common that it may make a little contribution to the prediction.\n",
        "# We consider to delete them by adding these features into our customized stop words.\n",
        "\n",
        "for word in feature_names[sorted_by_idf[:30]]:\n",
        "    delete_words.append(word)\n",
        "    \n",
        "    \n",
        " # again, we generate the features using TD-IDF\n",
        "vectorizer = TfidfVectorizer(stop_words = delete_words)\n",
        "vectorizer.fit(list_corpus)\n",
        "\n",
        "Xtrain = vectorizer.transform(data_train['ingre_string']).toarray()\n",
        "feature_names = np.array(vectorizer.get_feature_names())\n",
        "print(feature_names)\n",
        "print(len(feature_names)) # count how many feature we constructed by the bag of words model(TD-IDF).\n",
        "\n",
        "\n",
        "print(Xtrain)\n",
        "print(np.shape(Xtrain))\n",
        "print(np.array(ytrain))\n",
        "print(np.shape(ytrain))\n",
        "\n",
        "\n",
        "# we use this method to update our splited words and then combine them into a sentence once again.\n",
        "data_val = pd.DataFrame(columns=['ingredients'])\n",
        "data_val['ingredients'] = Xval \n",
        "data_val['ingre_splited'] = data_val['ingredients'].apply(tokenize_list)\n",
        "data_val['ingre_splited'] = data_val['ingre_splited'].apply(token_stemming)\n",
        "data_val['ingre_string'] = data_val['ingre_splited'].str.join(' ')\n",
        "Xval = vectorizer.transform(data_val['ingre_string']).toarray()\n",
        "\n",
        "\n",
        "data_test = pd.DataFrame(columns=['ingredients'])\n",
        "data_test['ingredients'] = Xtest \n",
        "data_test['ingre_splited'] = data_test['ingredients'].apply(tokenize_list)\n",
        "data_test['ingre_splited'] = data_test['ingre_splited'].apply(token_stemming)\n",
        "data_test['ingre_string'] = data_test['ingre_splited'].str.join(' ')\n",
        "Xtest = vectorizer.transform(data_test['ingre_string']).toarray()   \n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.decomposition import TruncatedSVD\n",
        "tsvd = TruncatedSVD(n_components=200, random_state = 42)\n",
        "\n",
        "Xtrain_svd = tsvd.fit(Xtrain).transform(Xtrain)\n",
        "Xtest_svd = tsvd.transform(Xtest)\n",
        "Xval_svd = tsvd.transform(Xval)\n",
        "\n",
        "print(Xtrain_svd)\n",
        "print(np.shape(Xtrain_svd))\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(39774, 4)\n",
            "{'southern_us': 3394, 'mexican': 5157, 'british': 636, 'filipino': 605, 'italian': 6209, 'thai': 1196, 'jamaican': 418, 'french': 2081, 'greek': 951, 'japanese': 1130, 'brazilian': 379, 'moroccan': 633, 'irish': 540, 'indian': 2417, 'spanish': 795, 'cajun_creole': 1224, 'chinese': 2163, 'vietnamese': 656, 'korean': 677, 'russian': 386}\n",
            "['10' '100' '14' ... 'ziti' 'zucchini' '√©pice']\n",
            "2426\n",
            "['salt' 'oil' 'pepper' 'garlic' 'onion' 'ground' 'oliv' 'sugar' 'sauc'\n",
            " 'tomato' 'chicken' 'water' 'chees' 'butter' 'flour' 'egg' 'clove'\n",
            " 'powder' 'chop' 'dri' 'juic' 'chili' 'veget' 'cilantro' 'milk' 'rice'\n",
            " 'cream' 'ginger' 'lemon' 'corn']\n",
            "['10' '100' '14' ... 'ziti' 'zucchini' '√©pice']\n",
            "2396\n",
            "[[0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " ...\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]]\n",
            "(31647, 2396)\n",
            "['southern_us' 'mexican' 'british' ... 'italian' 'japanese' 'southern_us']\n",
            "(31647,)\n",
            "[[ 0.19607073 -0.00058468 -0.08090434 ... -0.02948606 -0.00166915\n",
            "  -0.03063297]\n",
            " [ 0.240422   -0.07509746  0.0953415  ... -0.005765    0.0381716\n",
            "  -0.0183054 ]\n",
            " [ 0.16585297 -0.00294579  0.03090412 ...  0.00479029 -0.01218945\n",
            "  -0.01539773]\n",
            " ...\n",
            " [ 0.07391912  0.00557695 -0.06679217 ...  0.01828311 -0.00270692\n",
            "   0.04152147]\n",
            " [ 0.20047761 -0.07357975  0.13185743 ...  0.04664602  0.03466755\n",
            "  -0.00493774]\n",
            " [ 0.09681532  0.09695588 -0.01655168 ...  0.02914135  0.01934426\n",
            "   0.00659389]]\n",
            "(31647, 200)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "RUkUSKLg6ZiW",
        "colab_type": "code",
        "outputId": "c965a29f-b572-4b3e-de4c-0bae94f501a2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "cell_type": "code",
      "source": [
        "start = time.clock()\n",
        "clf = svm.SVC( gamma=0.2,C=0.7, decision_function_shape='ovo')\n",
        "clf.fit(Xtrain_svd, ytrain)\n",
        "elapsed = (time.clock() - start)\n",
        "print(\"Time used:\",elapsed)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Time used: 285.967134\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "2nm7wLmnAF3d",
        "colab_type": "code",
        "outputId": "fd3d38dd-aea7-4588-f867-188d2e1f02e9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "cell_type": "code",
      "source": [
        "start = time.clock()\n",
        "clf = LogisticRegression(random_state=0,solver='lbfgs',multi_class='multinomial',max_iter=1000).fit(Xtrain_svd,ytrain)\n",
        "elapsed = (time.clock() - start)\n",
        "print(\"Time used:\",elapsed)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Time used: 34.40326200000004\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "ZeXsfwzhETwx",
        "colab_type": "code",
        "outputId": "323b2dd4-81b2-47ef-93fe-f834397cdea8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        }
      },
      "cell_type": "code",
      "source": [
        "start = time.clock()\n",
        "clf = PassiveAggressiveClassifier(C=0.1,max_iter=2000, tol=1e-3,shuffle=False,)\n",
        "# clf.fit(Xtrain_svd, ytrain)\n",
        "\n",
        "print(Xtrain.shape)\n",
        "clf.fit(Xtrain, ytrain)\n",
        "\n",
        "elapsed = (time.clock() - start)\n",
        "print(\"Time used:\",elapsed)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(31647, 2396)\n",
            "Time used: 54.46043099999997\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "WUEHBAPU6cS6",
        "colab_type": "code",
        "outputId": "b874b7c5-c459-49e6-e461-c03609992079",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        }
      },
      "cell_type": "code",
      "source": [
        "p_result=clf.predict(Xtest)\n",
        "\n",
        "\n",
        "print(np.sum(ytest==p_result)/len(p_result))\n",
        "# print(np.bincount(yval))\n",
        "print(clf.score(Xtest,ytest))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.7591001011122346\n",
            "0.7591001011122346\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "tWS3gdZK_GIV",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "clf = LogisticRegression(random_state=0,solver='lbfgs',multi_class='multinomial').fit(X_train_svd,ytrain)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}